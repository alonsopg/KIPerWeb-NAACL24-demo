{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d42fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/user/question-retrieval-KIPerWeb/\")\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.corpus import stopwords\n",
    "from ranx import Qrels, evaluate, Run, compare\n",
    "import swifter\n",
    "import json\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from trectools import TrecPoolMaker, TrecRun\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635cce4",
   "metadata": {},
   "source": [
    "## Read the QB's sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d7d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read preprocessed data\n",
    "qb = pd.read_csv(\"/Users/user/question-retrieval-KIPerWeb/testbeds/samples/qb_stratified-sample.csv\").fillna(\"N/A\")\n",
    "qb = qb.drop(columns=['docid'])\n",
    "qb.rename(columns={\"category\":\"topic_label_de_fixed\"}, inplace=True)\n",
    "qb['docid'] = qb.index\n",
    "qb['content'] = qb['content'].apply(lambda x:remove_stop_words(x, 'german'))\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "# Models for preprocessing\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "qb['content'] = qb['content'].apply(lambda x : preprocess_documents(x, tagger_de))\n",
    "qb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qb['text'].iloc[3], qb['answers_content'].iloc[3])\n",
    "print(qb['content'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586d020",
   "metadata": {},
   "source": [
    "## Calculate Embeddings and Initialize NMSLIB Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fdc8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "model_2 = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\")\n",
    "# model_2.tokenizer.pad_token = model_2.tokenizer.eos_token\n",
    "\n",
    "model_3 = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "model_4 = SentenceTransformer(\"PM-AI/bi-encoder_msmarco_bert-base_german\")\n",
    "model_5 = SentenceTransformer(\"efederici/e5-base-multilingual-4096\")\n",
    "model_6 = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "model_7 = SentenceTransformer(\"clips/mfaq\")\n",
    "model_8 = SentenceTransformer(\"PM-AI/sts_paraphrase_xlm-roberta-base_de-en\")\n",
    "model_9 = SentenceTransformer(\"deutsche-telekom/gbert-large-paraphrase-euclidean\")\n",
    "model_10 = SentenceTransformer(\"LLukas22/all-MiniLM-L12-v2-embedding-all\")\n",
    "model_11 = SentenceTransformer(\"LLukas22/paraphrase-multilingual-mpnet-base-v2-embedding-all\")\n",
    "model_12 = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "model_13 = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "model_14 = SentenceTransformer(\"deutsche-telekom/gbert-large-paraphrase-cosine\")\n",
    "model_15 = SentenceTransformer(\"shibing624/text2vec-base-multilingual\")\n",
    "model_16 = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "model_17 = SentenceTransformer(\"setu4993/LaBSE\")\n",
    "model_18 = SentenceTransformer(\"symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\")\n",
    "model_19 = SentenceTransformer(\"and-effect/musterdatenkatalog_clf\")\n",
    "model_20 = SentenceTransformer(\"nblokker/debatenet-2-cat\")\n",
    "model_21 = SentenceTransformer(\"setu4993/LEALLA-large\")\n",
    "model_22 = SentenceTransformer(\"dell-research-harvard/lt-wikidata-comp-de\")\n",
    "model_23 = SentenceTransformer(\"ef-zulla/e5-multi-sml-torch\")\n",
    "model_24 = SentenceTransformer(\"barisaydin/text2vec-base-multilingual\")\n",
    "model_25 = SentenceTransformer(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model_25.tokenizer.pad_token = model_25.tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nmslib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def create_index(documents_set, model, **kwargs):\n",
    "    '''\n",
    "    Create index with TF-IDF weighted average embeddings.\n",
    "\n",
    "    documents_set: A dataframe with all the context to be indexed.\n",
    "    model: A language model to calculate embeddings.\n",
    "    index_path: The path for storing the index in case one needs to store it.\n",
    "    '''\n",
    "    # Initialize TF-IDF Vectorizer and compute TF-IDF matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents_set['content'].tolist())\n",
    "\n",
    "    # Initialize the nmslib index\n",
    "    index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "\n",
    "    # Function to compute TF-IDF weighted average embeddings\n",
    "    def tfidf_weighted_embeddings(text, vectorizer, tfidf_row):\n",
    "        # Tokenize text and get embeddings\n",
    "        words = text.split()\n",
    "        word_embeddings = model.encode(words, convert_to_tensor=True)\n",
    "\n",
    "        # Convert embeddings to numpy arrays if they are tensors\n",
    "        if hasattr(word_embeddings, 'cpu'):\n",
    "            word_embeddings = word_embeddings.cpu().numpy()\n",
    "\n",
    "        # Compute weights\n",
    "        weights = np.array([tfidf_row[0, vectorizer.vocabulary_.get(word.lower(), 0)] for word in words])\n",
    "\n",
    "        # Calculate weighted average\n",
    "        weighted_embeddings = word_embeddings * weights[:, np.newaxis]\n",
    "        weighted_average = np.sum(weighted_embeddings, axis=0) / np.sum(weights)\n",
    "        return weighted_average\n",
    "\n",
    "    # Compute TF-IDF weighted embeddings for each document\n",
    "    document_set_embeddings = []\n",
    "    for i, text in enumerate(documents_set['content'].tolist()):\n",
    "        weighted_avg_embeddings = tfidf_weighted_embeddings(text, tfidf_vectorizer, tfidf_matrix[i])\n",
    "        document_set_embeddings.append(weighted_avg_embeddings)\n",
    "\n",
    "    # Add data points to the index\n",
    "    index.addDataPointBatch(document_set_embeddings)\n",
    "    index.createIndex({'post': 2})\n",
    "\n",
    "    # Save index if path is provided\n",
    "    if 'index_path' in kwargs:\n",
    "        index.saveIndex(kwargs['index_path'])\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "index_1 = create_index(qb, model_1, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-paraphrase-multilingual-mpnet-base-v2.nmslib\")\n",
    "index_2 = create_index(qb, model_2, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/aari1995-German_Semantic_STS_V2.nmslib\")\n",
    "index_3 = create_index(qb, model_3, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-LaBSE.nmslib\")\n",
    "index_4 = create_index(qb, model_4, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/PM-AI-bi-encoder_msmarco_bert-base_german.nmslib\")\n",
    "index_5 = create_index(qb, model_5, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/efederici-e5-base-multilingual-4096.nmslib\")\n",
    "index_6 = create_index(qb, model_6, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/intfloat-multilingual-e5-base.nmslib\")\n",
    "index_7 = create_index(qb, model_7, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/clips-mfaq.nmslib\")\n",
    "index_8 = create_index(qb, model_8, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/PM-AI-sts_paraphrase_xlm-roberta-base_de-en.nmslib\")\n",
    "index_9 = create_index(qb, model_9, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/deutsche-telekom-gbert-large-paraphrase-euclidean.nmslib\")\n",
    "index_10 = create_index(qb, model_10, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/LLukas22-all-MiniLM-L12-v2-embedding-all.nmslib\")\n",
    "index_11 = create_index(qb, model_11, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/LLukas22-paraphrase-multilingual-mpnet-base-v2-embedding-all.nmslib\")\n",
    "index_12 = create_index(qb, model_12, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-distiluse-base-multilingual-cased-v1.nmslib\")\n",
    "index_13 = create_index(qb, model_13, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-distiluse-base-multilingual-cased-v2.nmslib\")\n",
    "index_14 = create_index(qb, model_14, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/deutsche-telekom-gbert-large-paraphrase-cosine.nmslib\")\n",
    "index_15 = create_index(qb, model_15, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/shibing624-text2vec-base-multilingual.nmslib\")\n",
    "index_16 = create_index(qb, model_16, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/Sahajtomar-German-semantic.nmslib\")\n",
    "index_17 = create_index(qb, model_17, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/setu4993-LaBSE.nmslib\")\n",
    "index_18 = create_index(qb, model_18, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/symanto-sn-xlm-roberta-base-snli-mnli-anli-xnli.nmslib\")\n",
    "index_19 = create_index(qb, model_19, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/and-effect-musterdatenkatalog_clf.nmslib\")\n",
    "index_20 = create_index(qb, model_20, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/nblokker-debatenet-2-cat.nmslib\")\n",
    "index_21 = create_index(qb, model_21, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/setu4993-LEALLA-large.nmslib\")\n",
    "index_22 = create_index(qb, model_22, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/dell-research-harvard-lt-wikidata-comp-de.nmslib\")\n",
    "index_23 = create_index(qb, model_23, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/ef-zulla-e5-multi-sml-torch.nmslib\")\n",
    "index_24 = create_index(qb, model_24, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/barisaydin-text2vec-base-multilingual.nmslib\")\n",
    "index_25 = create_index(qb, model_25, index_path=\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/meta-llama-Llama-2-7b-chat-hf.nmslib\")\n",
    "\n",
    "\n",
    "# # Or load them:\n",
    "\n",
    "# index_1 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-paraphrase-multilingual-mpnet-base-v2.nmslib\")\n",
    "# index_2 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/aari1995-German_Semantic_STS_V2.nmslib\")\n",
    "# index_3 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-LaBSE.nmslib\")\n",
    "# index_4 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/PM-AI-bi-encoder_msmarco_bert-base_german.nmslib\")\n",
    "# index_5 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/efederici-e5-base-multilingual-4096.nmslib\")\n",
    "# index_6 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/intfloat-multilingual-e5-base.nmslib\")\n",
    "# index_7 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/clips-mfaq.nmslib\")\n",
    "# index_8 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/PM-AI-sts_paraphrase_xlm-roberta-base_de-en.nmslib\")\n",
    "# index_9 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/deutsche-telekom-gbert-large-paraphrase-euclidean.nmslib\")\n",
    "# index_10 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/LLukas22-all-MiniLM-L12-v2-embedding-all.nmslib\")\n",
    "# index_11 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/LLukas22-paraphrase-multilingual-mpnet-base-v2-embedding-all.nmslib\")\n",
    "# index_12 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-distiluse-base-multilingual-cased-v1.nmslib\")\n",
    "# index_13 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/sentence-transformers-distiluse-base-multilingual-cased-v2.nmslib\")\n",
    "# index_14 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/deutsche-telekom-gbert-large-paraphrase-cosine.nmslib\")\n",
    "# index_15 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/shibing624-text2vec-base-multilingual.nmslib\")\n",
    "# index_16 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/Sahajtomar-German-semantic.nmslib\")\n",
    "# index_17 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/setu4993-LaBSE.nmslib\")\n",
    "# index_18 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/symanto-sn-xlm-roberta-base-snli-mnli-anli-xnli.nmslib\")\n",
    "# index_19 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/and-effect-musterdatenkatalog_clf.nmslib\")\n",
    "# index_20 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/nblokker-debatenet-2-cat.nmslib\")\n",
    "# index_21 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/setu4993-LEALLA-large.nmslib\")\n",
    "# index_22 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/dell-research-harvard-lt-wikidata-comp-de.nmslib\")\n",
    "# index_23 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/ef-zulla-e5-multi-sml-torch.nmslib\")\n",
    "# index_24 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/barisaydin-text2vec-base-multilingual.nmslib\")\n",
    "# index_25 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices-tfidf/meta-llama-Llama-2-7b-chat-hf.nmslib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7341b",
   "metadata": {},
   "source": [
    "## Run Queries Against Search Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac5abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def search(query, index, model, df, **kwargs):\n",
    "    \"\"\"\n",
    "    Approximate nearest neighbor search\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: A string query\n",
    "    index: An NMSLIB index\n",
    "    model: A language model\n",
    "    df: The content against the query will be runned\n",
    "    run_path: the path where the results will be stored (in JSON format)\n",
    "    k: The number of elements to be indexed by the search, by default is 10.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # Calculate embeddings with the language model\n",
    "    # try:\n",
    "    #     query_embeddings = model.encode(query)\n",
    "    # except ValueError:\n",
    "    #     # model.tokenizer.eos_token = model.tokenizer.eos_token\n",
    "    #     model.tokenizer.eos_token = \"[EOS]\"\n",
    "    #     model.tokenizer.mask_token = '[MASK]'\n",
    "    #     model.tokenizer.bos_token = '[BOS]'\n",
    "    #     # model.tokenizer.pad_token = model.tokenizer.pad_token_id\n",
    "    #     model.tokenizer.pad_token = '[PAD]'\n",
    "    #     model.tokenizer.cls_token = \"[CLS]\"\n",
    "    #     model.tokenizer.sep_token = \"[SEP]\"\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(df['content'].tolist())\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(df['content'].tolist())\n",
    "\n",
    "    # Function to compute TF-IDF weighted average embeddings\n",
    "    def tfidf_weighted_embeddings(text, vectorizer):\n",
    "        # Tokenize text and get embeddings\n",
    "        words = text.split()\n",
    "        word_embeddings = model.encode(words, convert_to_tensor=True)\n",
    "\n",
    "        # Compute weights\n",
    "        weights = torch.tensor([vectorizer.idf_[vectorizer.vocabulary_.get(word.lower(), 0)] for word in words])\n",
    "\n",
    "        # Perform the operations using PyTorch\n",
    "        if word_embeddings.dim() == 1:  # In case the result is a 1D tensor\n",
    "            word_embeddings = word_embeddings.unsqueeze(0)\n",
    "        if weights.dim() == 1:  # Also ensure weights are 2D\n",
    "            weights = weights.unsqueeze(0)\n",
    "        weighted_embeddings = word_embeddings * weights.T\n",
    "        weighted_average = torch.sum(weighted_embeddings, dim=0) / torch.sum(weights)\n",
    "        return weighted_average.cpu().numpy() if weighted_average.is_cuda else weighted_average.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    # query_embeddings = model.encode(query)\n",
    "    # Calculate TF-IDF weighted average embeddings for the query\n",
    "    query_embeddings = tfidf_weighted_embeddings(query, tfidf_vectorizer)\n",
    "\n",
    "\n",
    "    model_name = Path(str(model.tokenizer).split()[0].split(\"=\")[1].replace(\"'\",\"\").replace(\",\",\"\")).parts[-1:][0]\n",
    "    # With the index, make a query and approximate its 25 nearest neighbors\n",
    "    if 'k' in kwargs:\n",
    "        k = kwargs['k']\n",
    "    else:\n",
    "        k = df.shape[0]\n",
    "    ids, distances = index.knnQuery(query_embeddings, k=k)\n",
    "    # Process the output\n",
    "    indices_and_weights = list(zip(ids, distances))\n",
    "    \n",
    "    nmslib_indices = [e[0] for e in indices_and_weights]\n",
    "    results_df = df.iloc[nmslib_indices]\n",
    "    retrieved_question_ids = [e for e in results_df['docid'].tolist()]\n",
    "    # print(retrieved_question_ids)\n",
    "    # TODO: check if it is necessary to reset the indices in the DF,\n",
    "    # in the original DF there were repeated elements in the content column\n",
    "    # for consistency with the original data, the new reference id will be removed\n",
    "    results = df.query(\"docid in @retrieved_question_ids\")[:k]#.reset_index(drop=True)[:k]\n",
    "    # results['query_embedding'] = [query_embeddings]*results.shape[0] # this is for plotting, still needs to be reduced\n",
    "    results = results[['docid', 'onlinetest_title', 'question_type_id', 'question_type_name', \n",
    "                       'answer_type_id', 'answer_type_name', 'text', \n",
    "                       'correct_answers_temp','variable', 'points', 'answers',\n",
    "                       'source', 'level_difficulty', \n",
    "                       'topic_label_de_fixed', 'related_topics']]\n",
    "\n",
    "\n",
    "\n",
    "    results['answers'] = results['answers'].apply(to_dic)\n",
    "    results['related_topics'] = results['related_topics'].apply(to_dic)\n",
    "\n",
    "    \n",
    "    results['points'] = results['points'].fillna('N/A')\n",
    "    results['onlinetest_title'] = results['onlinetest_title'].fillna('N/A')\n",
    "    results['question_type_id'] = results['question_type_id'].fillna('N/A')\n",
    "    results['question_type_name'] = results['question_type_name'].fillna('N/A')\n",
    "    results['variable'] = results['variable'].fillna('N/A')\n",
    "    results['correct_answers_temp'] = results['correct_answers_temp'].fillna('N/A')\n",
    "\n",
    "    \n",
    "    topic_filters = list(set(results['topic_label_de_fixed'].tolist()))\n",
    "    answer_type_filters = list(set(results['answer_type_name'].tolist()))\n",
    "    question_type_filters = list(set(results['question_type_name'].tolist()))\n",
    "    question_difficulty_filters = list(set(results['level_difficulty'].tolist()))\n",
    "    source_filters = list(set(results['source'].tolist()))\n",
    "    points_filters = list(set(results['points'].tolist()))\n",
    "\n",
    "    results = [e[1] for e in results.T.to_dict().items()]\n",
    "    \n",
    "    for i, j in zip(results, indices_and_weights):\n",
    "        i['cosine_distance'] = str(j[1])\n",
    "    for e in results:\n",
    "        e['query']=str(query)\n",
    "\n",
    "    filter_data = {\n",
    "        'topic_filters':topic_filters, \n",
    "        'answer_type_filters': answer_type_filters,\n",
    "        'question_type_filters': question_type_filters,\n",
    "        'question_difficulty_filters': question_difficulty_filters,\n",
    "        'source_filters':source_filters, \n",
    "        'points_filters':points_filters\n",
    "        }\n",
    "    \n",
    "\n",
    "    results = {\n",
    "        'filter_data' : filter_data,\n",
    "        'search_output' : results\n",
    "        }\n",
    "\n",
    "    if 'run_path' in kwargs:\n",
    "        save_to_json([dict(e) for e in results['search_output']],\n",
    "                     kwargs['run_path'], model_name=model_name) # corregir este parseo ya no aplica!!!\n",
    "    else:\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query_test = search('Gesundheitswesen'.lower(), index_1, model_1, qb, k=200)['search_output']\n",
    "query_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We could use queries:\n",
    "# queries_file = pd.read_csv(\"/Users/user/question-retrieval-KIPerWeb/testbeds/topics_experiments/topics/topics.csv\")\n",
    "queries_file = pd.read_csv(\"/Users/user/question-retrieval-KIPerWeb/testbeds/queries_experiments/queries/queries.csv\")\n",
    "queries_file['queries'] = queries_file['queries'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-> Number of queries:\",len(queries_file['queries'].tolist()))\n",
    "\n",
    "queries = queries_file['queries'].tolist()\n",
    "query_ids = queries_file['qid'].tolist()\n",
    "queries_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e99c8",
   "metadata": {},
   "source": [
    "## Generate searches and store them as JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44349041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/ANN_LM-TFIDF-Weighted-AVG/json_format/'\n",
    "progress_bar = tqdm(zip(queries, query_ids), total=len(queries), desc=\"Processing\")\n",
    "\n",
    "for i, j in progress_bar:\n",
    "    progress_bar.set_description(f\"Processing query: {i}\")\n",
    "    search(i, index_1, model_1, qb, k=100, run_path=f'{path}query_{j}/run_01.json')\n",
    "    search(i, index_2, model_2, qb, k=100, run_path=f'{path}query_{j}/run_02.json')\n",
    "    search(i, index_3, model_3, qb, k=100, run_path=f'{path}query_{j}/run_03.json')\n",
    "    search(i, index_4, model_4, qb, k=100, run_path=f'{path}query_{j}/run_04.json')\n",
    "    search(i, index_5, model_5, qb, k=100, run_path=f'{path}query_{j}/run_05.json')\n",
    "    search(i, index_6, model_6, qb, k=100, run_path=f'{path}query_{j}/run_06.json')\n",
    "    search(i, index_7, model_7, qb, k=100, run_path=f'{path}query_{j}/run_07.json')\n",
    "    search(i, index_8, model_8, qb, k=100, run_path=f'{path}query_{j}/run_08.json')\n",
    "    search(i, index_9, model_9, qb, k=100, run_path=f'{path}query_{j}/run_09.json')\n",
    "    search(i, index_10, model_10, qb, k=100, run_path=f'{path}query_{j}/run_10.json')\n",
    "    search(i, index_11, model_11, qb, k=100, run_path=f'{path}query_{j}/run_11.json')\n",
    "    search(i, index_12, model_12, qb, k=100, run_path=f'{path}query_{j}/run_12.json')\n",
    "    search(i, index_13, model_13, qb, k=100, run_path=f'{path}query_{j}/run_13.json')\n",
    "    search(i, index_14, model_14, qb, k=100, run_path=f'{path}query_{j}/run_14.json')\n",
    "    search(i, index_15, model_15, qb, k=100, run_path=f'{path}query_{j}/run_15.json')\n",
    "    search(i, index_16, model_16, qb, k=100, run_path=f'{path}query_{j}/run_16.json')\n",
    "    search(i, index_17, model_17, qb, k=100, run_path=f'{path}query_{j}/run_17.json')\n",
    "    search(i, index_18, model_18, qb, k=100, run_path=f'{path}query_{j}/run_18.json')\n",
    "    search(i, index_19, model_19, qb, k=100, run_path=f'{path}query_{j}/run_19.json')\n",
    "    search(i, index_20, model_20, qb, k=100, run_path=f'{path}query_{j}/run_20.json')\n",
    "    search(i, index_21, model_21, qb, k=100, run_path=f'{path}query_{j}/run_21.json')\n",
    "    search(i, index_22, model_22, qb, k=100, run_path=f'{path}query_{j}/run_22.json')\n",
    "    search(i, index_23, model_23, qb, k=100, run_path=f'{path}query_{j}/run_23.json')\n",
    "    search(i, index_24, model_24, qb, k=100, run_path=f'{path}query_{j}/run_24.json')\n",
    "    search(i, index_25, model_25, qb, k=100, run_path=f'{path}query_{j}/run_25.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77956c6e",
   "metadata": {},
   "source": [
    "## Preprocess all the synthetic searches and turn them into TREC style files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bacb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the JSON outputs of each run\n",
    "\n",
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/ANN_LM-TFIDF-Weighted-AVG/json_format/'\n",
    "json_files = list_files_from_dir(path, extention='.json')\n",
    "# json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_length(list_of_files):\n",
    "#     assert len(list_of_files) == 375, print(len(list_of_files))\n",
    "#     for e in list_of_files:\n",
    "#         assert 100 == len(read_json(e)), print(f\"{e} -> didnt match the a length of 100, it returned less:\", len(read_json(e)))\n",
    "    \n",
    "        \n",
    "# validate_length(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd3313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each JSON output, transform the results to a TREC format\n",
    "\n",
    "for e in json_files:\n",
    "    try:\n",
    "        to_trec_format(e)\n",
    "    except IndexError:\n",
    "        print(f\"Error! in {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the TREC formatted files, these have a .tsv extention\n",
    "list_files_from_dir(path, extention='.tsv')\n",
    "\n",
    "# Process all the runs and continue with the trec formatting\n",
    "path_list_processed = list_files_from_dir(path, extention='.tsv')\n",
    "\n",
    "list_of_dfs = []\n",
    "for e in path_list_processed:\n",
    "    df = pd.concat([pd.read_csv(e, header=None, sep='\\t', names=[\"queries\", \"Q0\", \"docid\",\n",
    "                                                                 \"rank\", \"score\", \"tag\"])])\n",
    "    list_of_dfs.append(df)\n",
    "    \n",
    "\n",
    "\n",
    "all_runs = pd.concat(list_of_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "all_runs= all_runs.merge(queries_file,on='queries')\n",
    "# all_runs = all_runs[[\"qid\", \"Q0\", \"docid\", \"rank\", \"score\", \"tag\"]]\n",
    "all_runs = all_runs[[\"qid\", \"Q0\", \"docid\", \"score\", \"rank\", \"tag\"]]\n",
    "\n",
    "\n",
    "all_runs['qid'] = all_runs['qid'].astype(str)\n",
    "all_runs['docid'] = all_runs['docid'].astype(str)\n",
    "\n",
    "all_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract runs by different LMs\n",
    "queries = list(set(all_runs['qid']))\n",
    "L_models = list(set(all_runs['tag']))\n",
    "runs = [all_runs.query(f'tag==\"{e}\"') for e in L_models]\n",
    "\n",
    "for i, j in zip(runs, L_models):\n",
    "    i.to_csv(f'/Users/user/question-retrieval-KIPerWeb/runs/ANN_LM-TFIDF-Weighted-AVG/trec_format/{j}_run.txt', header = None, sep='\\t',index=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"/Users/user/question-retrieval-KIPerWeb/testbeds/queries_experiments/trec_pools/testbed.csv\"\n",
    "\n",
    "qrels_df = pd.read_csv(f)\n",
    "qrels_df['qid'] = qrels_df['qid'].astype(str)\n",
    "qrels_df['docid'] = qrels_df['docid'].astype(str)\n",
    "\n",
    "qrels = Qrels.from_df(\n",
    "    df=qrels_df,\n",
    "    q_id_col=\"qid\",\n",
    "    doc_id_col=\"docid\",\n",
    "    score_col=\"qrel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/ANN_LM-TFIDF-Weighted-AVG/trec_format/'\n",
    "trec_runs = list_files_from_dir(path, extention='.txt')\n",
    "trec_runs\n",
    "\n",
    "runs = []\n",
    "for e in trec_runs:\n",
    "    runs.append(Run.from_file(e))\n",
    "    \n",
    "runs_names = []\n",
    "for e in trec_runs:\n",
    "    runs_names.append(get_last_element_of_path(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484cc2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = create_report(qrels, runs, runs_names, 25,\"/Users/user/question-retrieval-KIPerWeb/data/results/ANN_LM-TFIDF-Weighted-AVG/ann_tfidf_avg_metrics_at_25.csv\")\n",
    "report.sort_values(by='ndcg@25',  ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491788d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = create_report(qrels, runs, runs_names, 50,\"/Users/user/question-retrieval-KIPerWeb/data/results/ANN_LM-TFIDF-Weighted-AVG/ann_tfidf_avg_metrics_at_50.csv\")\n",
    "report.sort_values(by='ndcg@50',  ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2288877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = create_report(qrels, runs, runs_names, 100,\"/Users/user/question-retrieval-KIPerWeb/data/results/ANN_LM-TFIDF-Weighted-AVG/ann_tfidf_avg_metrics_at_100.csv\")\n",
    "report.sort_values(by='ndcg@100',  ascending=False).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
