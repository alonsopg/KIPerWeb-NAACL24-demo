{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249f1a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/user/question-retrieval-KIPerWeb/\")\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.corpus import stopwords\n",
    "from ranx import Qrels, evaluate, Run\n",
    "import swifter\n",
    "import json\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from trectools import TrecPoolMaker, TrecRun\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ed339",
   "metadata": {},
   "source": [
    "## Read the QB's sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b82f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read preprocessed data\n",
    "qb = pd.read_csv(\"/Users/user/question-retrieval-KIPerWeb/testbeds/samples/qb_stratified-sample.csv\").fillna(\"N/A\")\n",
    "qb = qb.drop(columns=['docid'])\n",
    "qb.rename(columns={\"category\":\"topic_label_de_fixed\"}, inplace=True)\n",
    "qb['docid'] = qb.index\n",
    "qb['content'] = qb['content'].apply(lambda x:remove_stop_words(x, 'german'))\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "# Models for preprocessing\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "tagger_de = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "\n",
    "qb['content'] = qb['content'].apply(lambda x : preprocess_documents(x, tagger_de))\n",
    "qb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79abcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qb['text'].iloc[3], qb['answers_content'].iloc[3])\n",
    "\n",
    "print(qb['content'].iloc[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec79340",
   "metadata": {},
   "source": [
    "## Calculate Embeddings and Initialize NMSLIB Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c919b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "model_2 = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\")\n",
    "model_3 = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "model_4 = SentenceTransformer(\"PM-AI/bi-encoder_msmarco_bert-base_german\")\n",
    "model_5 = SentenceTransformer(\"efederici/e5-base-multilingual-4096\")\n",
    "model_6 = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "model_7 = SentenceTransformer(\"clips/mfaq\")\n",
    "model_8 = SentenceTransformer(\"PM-AI/sts_paraphrase_xlm-roberta-base_de-en\")\n",
    "model_9 = SentenceTransformer(\"deutsche-telekom/gbert-large-paraphrase-euclidean\")\n",
    "model_10 = SentenceTransformer(\"LLukas22/all-MiniLM-L12-v2-embedding-all\")\n",
    "model_11 = SentenceTransformer(\"LLukas22/paraphrase-multilingual-mpnet-base-v2-embedding-all\")\n",
    "model_12 = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v1\")\n",
    "model_13 = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "model_14 = SentenceTransformer(\"deutsche-telekom/gbert-large-paraphrase-cosine\")\n",
    "model_15 = SentenceTransformer(\"shibing624/text2vec-base-multilingual\")\n",
    "model_16 = SentenceTransformer(\"Sahajtomar/German-semantic\")\n",
    "model_17 = SentenceTransformer(\"setu4993/LaBSE\")\n",
    "model_18 = SentenceTransformer(\"symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\")\n",
    "model_19 = SentenceTransformer(\"and-effect/musterdatenkatalog_clf\")\n",
    "model_20 = SentenceTransformer(\"nblokker/debatenet-2-cat\")\n",
    "model_21 = SentenceTransformer(\"setu4993/LEALLA-large\")\n",
    "model_22 = SentenceTransformer(\"dell-research-harvard/lt-wikidata-comp-de\")\n",
    "model_23 = SentenceTransformer(\"ef-zulla/e5-multi-sml-torch\")\n",
    "model_24 = SentenceTransformer(\"barisaydin/text2vec-base-multilingual\")\n",
    "model_25 = SentenceTransformer(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model_25.tokenizer.pad_token = model_25.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_1 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/sentence-transformers-paraphrase-multilingual-mpnet-base-v2.nmslib\")\n",
    "index_2 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/aari1995-German_Semantic_STS_V2.nmslib\")\n",
    "index_3 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/sentence-transformers-LaBSE.nmslib\")\n",
    "index_4 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/PM-AI-bi-encoder_msmarco_bert-base_german.nmslib\")\n",
    "index_5 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/efederici-e5-base-multilingual-4096.nmslib\")\n",
    "index_6 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/intfloat-multilingual-e5-base.nmslib\")\n",
    "index_7 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/clips-mfaq.nmslib\")\n",
    "index_8 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/PM-AI-sts_paraphrase_xlm-roberta-base_de-en.nmslib\")\n",
    "index_9 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/deutsche-telekom-gbert-large-paraphrase-euclidean.nmslib\")\n",
    "index_10 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/LLukas22-all-MiniLM-L12-v2-embedding-all.nmslib\")\n",
    "index_11 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/LLukas22-paraphrase-multilingual-mpnet-base-v2-embedding-all.nmslib\")\n",
    "index_12 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/sentence-transformers-distiluse-base-multilingual-cased-v1.nmslib\")\n",
    "index_13 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/sentence-transformers-distiluse-base-multilingual-cased-v2.nmslib\")\n",
    "index_14 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/deutsche-telekom-gbert-large-paraphrase-cosine.nmslib\")\n",
    "index_15 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/shibing624-text2vec-base-multilingual.nmslib\")\n",
    "index_16 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/Sahajtomar-German-semantic.nmslib\")\n",
    "index_17 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/setu4993-LaBSE.nmslib\")\n",
    "index_18 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/symanto-sn-xlm-roberta-base-snli-mnli-anli-xnli.nmslib\")\n",
    "index_19 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/and-effect-musterdatenkatalog_clf.nmslib\")\n",
    "index_20 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/nblokker-debatenet-2-cat.nmslib\")\n",
    "index_21 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/setu4993-LEALLA-large.nmslib\")\n",
    "index_22 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/dell-research-harvard-lt-wikidata-comp-de.nmslib\")\n",
    "index_23 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/ef-zulla-e5-multi-sml-torch.nmslib\")\n",
    "index_24 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/barisaydin-text2vec-base-multilingual.nmslib\")\n",
    "index_25 = load_nmslib_index(\"/Users/user/question-retrieval-KIPerWeb/nmslib_indices/meta-llama-Llama-2-7b-chat-hf.nmslib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21769f0c",
   "metadata": {},
   "source": [
    "## Run Queries Against Search Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726b277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_test = search('Gesundheitswesen'.lower(), index_11, model_11, qb, k=100)['search_output']\n",
    "# query_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f59068",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_file = pd.read_csv(\"/Users/user/question-retrieval-KIPerWeb/testbeds/queries_experiments/queries/queries.csv\")\n",
    "queries_file['queries'] = queries_file['queries'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-> Number of queries:\",len(queries_file['queries'].tolist()))\n",
    "\n",
    "queries = queries_file['queries'].tolist()\n",
    "query_ids = queries_file['qid'].tolist()\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84d316",
   "metadata": {},
   "source": [
    "## Apply a vertical search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(q, q_type, model):\n",
    "    return util.cos_sim(encode_utterance(q, model),encode_utterance(q_type, model)).numpy()[0][0]\n",
    "\n",
    "\n",
    "def encode_utterance(utterance, model):\n",
    "    return model.encode(utterance, convert_to_tensor=False)\n",
    "\n",
    "\n",
    "def restrict_search(query, df, model):\n",
    "    topic_labels = list(set(df['topic_label_de_fixed'].tolist()))\n",
    "    pairs = list(itertools.product([query], topic_labels))\n",
    "    pairs_w_cos = []\n",
    "    for e in pairs:\n",
    "        pairs_w_cos.append((e[0], e[1], get_cosine_sim(e[0], e[1], model)))\n",
    "\n",
    "    restricted_search = sorted(pairs_w_cos, key=operator.itemgetter(2), reverse=True)[0:15]\n",
    "    restricted_topics = [e[1] for e in restricted_search]\n",
    "    df = df.query(\"topic_label_de_fixed in @restricted_topics\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_index(documents_set, model, **kwargs):\n",
    "    '''\n",
    "    Create index\n",
    "    \n",
    "    documents_set: A dataframe with all the context to be indexed.\n",
    "    model: A language model to calculate embeddings.\n",
    "    index_path: The path for storing the index in case one needs to store it.\n",
    "    '''\n",
    "    index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    # model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "    # model.tokenizer.mask_token = '[MASK]' # or another appropriate token\n",
    "    if model.tokenizer.pad_token is None:\n",
    "    # Check if the tokenizer has a default pad token\n",
    "        if hasattr(model.tokenizer, 'pad_token_id'):\n",
    "            model.tokenizer.pad_token = model.tokenizer.pad_token_id\n",
    "        else:\n",
    "            # Set a standard pad token if the tokenizer supports it\n",
    "            # This part might need adjustment based on the specific tokenizer\n",
    "            model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "    try:\n",
    "        document_set_embeddings = [model.encode(e) for e in documents_set]\n",
    "    except ValueError:\n",
    "        model.tokenizer.pad_token = model.tokenizer.eos_token\n",
    "        model.tokenizer.mask_token = '[MASK]' # or another appropriate token\n",
    "        document_set_embeddings = [model.encode(e) for e in documents_set]\n",
    "        \n",
    "    \n",
    "    \n",
    "    index.addDataPointBatch(document_set_embeddings)\n",
    "    index.createIndex({'M': 200, 'efConstruction': 200, 'post': 2, 'searchMethod':200})\n",
    "\n",
    "    if 'index_path' in kwargs:\n",
    "        index.saveIndex(kwargs['index_path'])\n",
    "    else:\n",
    "        pass\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def vert_search(query, index, model, df, **kwargs):\n",
    "    \"\"\"\n",
    "    Approximate nearest neighbor search\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: A string query\n",
    "    index: An NMSLIB index\n",
    "    model: A language model\n",
    "    df: The content against the query will be runned\n",
    "    run_path: the path where the results will be stored (in JSON format)\n",
    "    k: The number of elements to be indexed by the search, by default is 10.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if model.tokenizer.eos_token is None:\n",
    "        model.tokenizer.eos_token = \"[EOS]\"\n",
    "\n",
    "    if model.tokenizer.mask_token is None:\n",
    "        model.tokenizer.mask_token = \"[MASK]\"\n",
    "\n",
    "    if model.tokenizer.bos_token is None:\n",
    "        model.tokenizer.bos_token = \"[BOS]\"\n",
    "\n",
    "    if model.tokenizer.pad_token is None:\n",
    "        model.tokenizer.pad_token = \"[PAD]\"\n",
    "\n",
    "    if model.tokenizer.cls_token is None:\n",
    "        model.tokenizer.cls_token = \"[CLS]\"\n",
    "\n",
    "    if model.tokenizer.sep_token is None:\n",
    "        model.tokenizer.sep_token = \"[SEP]\"\n",
    "\n",
    "    query_embeddings = model.encode(query)\n",
    "\n",
    "    model_name = Path(str(model.tokenizer).split()[0].split(\"=\")[1].replace(\"'\",\"\").replace(\",\",\"\")).parts[-1:][0]\n",
    "    # With the index, make a query and approximate its 25 nearest neighbors\n",
    "    if 'k' in kwargs:\n",
    "        k = kwargs['k']\n",
    "    else:\n",
    "        k = 10\n",
    "        \n",
    "#     print(\"original df\",df.shape[0])\n",
    "#     print(\"restricting to:\", query)\n",
    "    df = restrict_search(query, df, model)\n",
    "#     print(\"restricted df\", df.shape[0])\n",
    "    \n",
    "    documents_set = df['content'].tolist()\n",
    "#     print(\"started creating temporary index...\")    \n",
    "    index = create_index(documents_set, model)\n",
    "#     print(\"finished creating temporary index...\")\n",
    "    \n",
    "    ids, distances = index.knnQuery(query_embeddings, k=k)\n",
    "    # Process the output\n",
    "    indices_and_weights = list(zip(ids, distances))\n",
    "#     print('ids',ids)\n",
    "    nmslib_indices = [e[0] for e in indices_and_weights]\n",
    "    results_df = df.iloc[nmslib_indices]\n",
    "    retrieved_question_ids = [e for e in results_df['docid'].tolist()]\n",
    "\n",
    "    results = df.query(\"docid in @retrieved_question_ids\")[:k]#.reset_index(drop=True)[:k]\n",
    "    results = results[['docid', 'onlinetest_title', 'question_type_id', 'question_type_name', \n",
    "                       'answer_type_id', 'answer_type_name', 'text', \n",
    "                       'correct_answers_temp','variable', 'points', 'answers',\n",
    "                       'source', 'level_difficulty', 'topic_label_de_fixed', 'related_topics']]\n",
    "\n",
    "\n",
    "\n",
    "    results['answers'] = results['answers'].apply(to_dic)\n",
    "    results['related_topics'] = results['related_topics'].apply(to_dic)\n",
    "\n",
    "    \n",
    "    results['points'] = results['points'].fillna('N/A')\n",
    "    results['onlinetest_title'] = results['onlinetest_title'].fillna('N/A')\n",
    "    results['question_type_id'] = results['question_type_id'].fillna('N/A')\n",
    "    results['question_type_name'] = results['question_type_name'].fillna('N/A')\n",
    "    results['variable'] = results['variable'].fillna('N/A')\n",
    "    results['correct_answers_temp'] = results['correct_answers_temp'].fillna('N/A')\n",
    "\n",
    "    \n",
    "    topic_filters = list(set(results['topic_label_de_fixed'].tolist()))\n",
    "    answer_type_filters = list(set(results['answer_type_name'].tolist()))\n",
    "    question_type_filters = list(set(results['question_type_name'].tolist()))\n",
    "    question_difficulty_filters = list(set(results['level_difficulty'].tolist()))\n",
    "    source_filters = list(set(results['source'].tolist()))\n",
    "    points_filters = list(set(results['points'].tolist()))\n",
    "\n",
    "    results = [e[1] for e in results.T.to_dict().items()]\n",
    "    \n",
    "    for i, j in zip(results, indices_and_weights):\n",
    "        i['cosine_distance'] = str(j[1])\n",
    "    for e in results:\n",
    "        e['query']=str(query)\n",
    "\n",
    "    filter_data = {\n",
    "        'topic_filters':topic_filters, \n",
    "        'answer_type_filters': answer_type_filters,\n",
    "        'question_type_filters': question_type_filters,\n",
    "        'question_difficulty_filters': question_difficulty_filters,\n",
    "        'source_filters':source_filters, \n",
    "        'points_filters':points_filters\n",
    "        }\n",
    "    \n",
    "\n",
    "    results = {\n",
    "        'filter_data' : filter_data,\n",
    "        'search_output' : results\n",
    "        }\n",
    "\n",
    "    if 'run_path' in kwargs:\n",
    "        save_to_json([dict(e) for e in results['search_output']], kwargs['run_path'], model_name=model_name) # corregir este parseo ya no aplica!!!\n",
    "    else:\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6a524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/vertical_search/json_format/'\n",
    "\n",
    "for i, j in zip(queries, query_ids):\n",
    "    vert_search(i.lower(), index_1, model_1, qb, k=100, run_path=f'{path}query_{j}/run_01.json')\n",
    "    vert_search(i.lower(), index_2, model_2, qb, k=100, run_path=f'{path}query_{j}/run_02.json')\n",
    "    vert_search(i.lower(), index_3, model_3, qb, k=100, run_path=f'{path}query_{j}/run_03.json')\n",
    "    vert_search(i.lower(), index_4, model_4, qb, k=100, run_path=f'{path}query_{j}/run_04.json')\n",
    "    vert_search(i.lower(), index_5, model_5, qb, k=100, run_path=f'{path}query_{j}/run_05.json')\n",
    "    vert_search(i.lower(), index_6, model_6, qb, k=100, run_path=f'{path}query_{j}/run_06.json')\n",
    "    vert_search(i.lower(), index_7, model_7, qb, k=100, run_path=f'{path}query_{j}/run_07.json')\n",
    "    vert_search(i.lower(), index_8, model_8, qb, k=100, run_path=f'{path}query_{j}/run_08.json')\n",
    "    vert_search(i.lower(), index_9, model_9, qb, k=100, run_path=f'{path}query_{j}/run_09.json')\n",
    "    vert_search(i.lower(), index_10, model_10, qb, k=100, run_path=f'{path}query_{j}/run_10.json')\n",
    "    vert_search(i.lower(), index_11, model_11, qb, k=100, run_path=f'{path}query_{j}/run_11.json')\n",
    "    vert_search(i.lower(), index_12, model_12, qb, k=100, run_path=f'{path}query_{j}/run_12.json')\n",
    "    vert_search(i.lower(), index_13, model_13, qb, k=100, run_path=f'{path}query_{j}/run_13.json')\n",
    "    vert_search(i.lower(), index_14, model_14, qb, k=100, run_path=f'{path}query_{j}/run_14.json')\n",
    "    vert_search(i.lower(), index_15, model_15, qb, k=100, run_path=f'{path}query_{j}/run_15.json')\n",
    "    vert_search(i.lower(), index_16, model_16, qb, k=100, run_path=f'{path}query_{j}/run_16.json')\n",
    "    vert_search(i.lower(), index_17, model_17, qb, k=100, run_path=f'{path}query_{j}/run_17.json')\n",
    "    vert_search(i.lower(), index_18, model_18, qb, k=100, run_path=f'{path}query_{j}/run_18.json')\n",
    "    vert_search(i.lower(), index_19, model_19, qb, k=100, run_path=f'{path}query_{j}/run_19.json')\n",
    "    vert_search(i.lower(), index_20, model_20, qb, k=100, run_path=f'{path}query_{j}/run_20.json')\n",
    "    vert_search(i.lower(), index_21, model_21, qb, k=100, run_path=f'{path}query_{j}/run_21.json')\n",
    "    vert_search(i.lower(), index_22, model_22, qb, k=100, run_path=f'{path}query_{j}/run_22.json')\n",
    "    vert_search(i.lower(), index_23, model_23, qb, k=100, run_path=f'{path}query_{j}/run_23.json')\n",
    "    vert_search(i.lower(), index_24, model_24, qb, k=100, run_path=f'{path}query_{j}/run_24.json')\n",
    "    vert_search(i.lower(), index_25, model_25, qb, k=100, run_path=f'{path}query_{j}/run_25.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e50b55",
   "metadata": {},
   "source": [
    "## Preprocess all the synthetic searches and turn them into TREC style files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f910a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all the JSON outputs of each run\n",
    "\n",
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/vertical_search/json_format/'\n",
    "json_files = list_files_from_dir(path, extention='.json')\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba550677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each JSON output, transform the results to a TREC format\n",
    "\n",
    "for e in json_files:\n",
    "    try:\n",
    "        to_trec_format(e)\n",
    "    except IndexError:\n",
    "        print(f\"Error! in {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf41d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the TREC formatted files, these have a .tsv extention\n",
    "list_files_from_dir(path, extention='.tsv')\n",
    "\n",
    "# Process all the runs and continue with the trec formatting\n",
    "path_list_processed = list_files_from_dir(path, extention='.tsv')\n",
    "\n",
    "list_of_dfs = []\n",
    "for e in path_list_processed:\n",
    "    df = pd.concat([pd.read_csv(e, header=None, sep='\\t', \n",
    "                                names=[\"queries\", \"Q0\", \"docid\", \"rank\", \"score\", \"tag\"])])\n",
    "    list_of_dfs.append(df)\n",
    "    \n",
    "\n",
    "\n",
    "all_runs = pd.concat(list_of_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "all_runs= all_runs.merge(queries_file,on='queries')\n",
    "all_runs = all_runs[[\"qid\", \"Q0\", \"docid\", \"score\", \"rank\", \"tag\"]]\n",
    "\n",
    "all_runs['qid'] = all_runs['qid'].astype(str)\n",
    "all_runs['docid'] = all_runs['docid'].astype(str)\n",
    "all_runs['tag'] = all_runs['tag'].apply(lambda x: f'vertical-{x}')\n",
    "\n",
    "all_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract runs by different LMs\n",
    "queries = list(set(all_runs['qid']))\n",
    "L_models = list(set(all_runs['tag']))\n",
    "runs = [all_runs.query(f'tag==\"{e}\"') for e in L_models]\n",
    "\n",
    "for i, j in zip(runs, L_models):\n",
    "    i.to_csv(f'/Users/user/question-retrieval-KIPerWeb/runs/vertical_search/trec_format/{j}_run.txt', header = None, sep='\\t',index=False )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ef7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"/Users/user/question-retrieval-KIPerWeb/testbeds/queries_experiments/trec_pools/testbed.csv\"\n",
    "\n",
    "qrels_df = pd.read_csv(f)\n",
    "qrels_df['qid'] = qrels_df['qid'].astype(str)\n",
    "qrels_df['docid'] = qrels_df['docid'].astype(str)\n",
    "\n",
    "qrels = Qrels.from_df(\n",
    "    df=qrels_df,\n",
    "    q_id_col=\"qid\",\n",
    "    doc_id_col=\"docid\",\n",
    "    score_col=\"qrel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf58d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/user/question-retrieval-KIPerWeb/runs/vertical_search/trec_format/'\n",
    "trec_runs = list_files_from_dir(path, extention='.txt')\n",
    "trec_runs\n",
    "\n",
    "runs = []\n",
    "for e in trec_runs:\n",
    "    runs.append(Run.from_file(e))\n",
    "    \n",
    "runs_names = []\n",
    "for e in trec_runs:\n",
    "    runs_names.append(get_last_element_of_path(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "\n",
    "report = create_report(qrels, runs, runs_names, k,f\"/Users/user/question-retrieval-KIPerWeb/data/results/vertical_search/vertical_search_at_{k}.csv\")\n",
    "report.sort_values(by='mrr@25',  ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb433d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "report = create_report(qrels, runs, runs_names, k,f\"/Users/user/question-retrieval-KIPerWeb/data/results/vertical_search/vertical_search_at_{k}.csv\")\n",
    "report.sort_values(by=f'mrr@{k}',  ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6288ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "\n",
    "report = create_report(qrels, runs, runs_names, k,f\"/Users/user/question-retrieval-KIPerWeb/data/results/vertical_search/vertical_search_at_{k}.csv\")\n",
    "report.sort_values(by=f'ndcg@{k}',  ascending=False).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
